{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting triton==2.0.0\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/jun/miniconda3/envs/mne/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/jun/miniconda3/envs/mne/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: wheel in /home/jun/miniconda3/envs/mne/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/jun/miniconda3/envs/mne/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (66.0.0)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.5.tar.gz (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Using cached cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jun/miniconda3/envs/mne/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.5-py3-none-any.whl size=88174 sha256=f9507b2cc4919571b61665bf5d3eca09cd7c1951cef7d73212d5c28be0c55cbe\n",
      "  Stored in directory: /home/jun/.cache/pip/wheels/eb/02/84/d82f0b1a6098209edf7e3607be6cc592ebbc015a8a3127c68d\n",
      "Successfully built lit\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.3 filelock-3.12.0 lit-16.0.5 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/1000, Loss: 0.11548864096403122\n",
      "Epoch: 200/1000, Loss: 0.11048425734043121\n",
      "Epoch: 300/1000, Loss: 0.10723289847373962\n",
      "Epoch: 400/1000, Loss: 0.10425393283367157\n",
      "Epoch: 500/1000, Loss: 0.10151467472314835\n",
      "Epoch: 600/1000, Loss: 0.09899371862411499\n",
      "Epoch: 700/1000, Loss: 0.09667181223630905\n",
      "Epoch: 800/1000, Loss: 0.09453147649765015\n",
      "Epoch: 900/1000, Loss: 0.092556893825531\n",
      "Epoch: 1000/1000, Loss: 0.09073369950056076\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network architecture for the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_features, num_components):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, num_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "# Prepare the input and output data\n",
    "# Assuming you have Y (input data) and X (expected output) as NumPy arrays\n",
    "Y = np.random.rand(100, 20)  # Example input data (100 samples, 20 features)\n",
    "X = np.random.rand(100, 5)   # Example expected output (100 samples, 5 components)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Define the configuration (similar to cfg dictionary in the provided code)\n",
    "cfg = {\n",
    "    'gamma': 0.1,\n",
    "    'returnPattern': False,\n",
    "    'demean': True\n",
    "}\n",
    "\n",
    "# Train the encoder\n",
    "num_features = Y.shape[1]\n",
    "num_components = X.shape[1]\n",
    "\n",
    "# Create an instance of the Encoder network\n",
    "encoder = Encoder(num_features, num_components)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = encoder(Y_tensor)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, X_tensor)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for monitoring the training progress\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Access the learned weights of the encoder\n",
    "encoder_weights = encoder.fc.weight.detach().numpy()\n",
    "\n",
    "# Access the learned patterns (if required)\n",
    "if cfg['returnPattern']:\n",
    "    encoder_patterns = encoder_weights.T  # Transpose the weight matrix if necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.random.rand(100, 20)\n",
    "new_data_tensor = torch.tensor(new_data, dtype=torch.float32)\n",
    "encoded_output = encoder(new_data_tensor).detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "[[0.07627269 0.55635642 0.50809574 ... 0.00362764 0.36955551 0.4265542 ]\n",
      " [0.81776772 0.48315676 0.133133   ... 0.3152899  0.42362307 0.97148497]\n",
      " [0.03306886 0.34173603 0.76609811 ... 0.68346859 0.17852321 0.45351997]\n",
      " ...\n",
      " [0.49954603 0.26662369 0.91547228 ... 0.20801754 0.57939769 0.33875207]\n",
      " [0.19761672 0.03520723 0.75738913 ... 0.73528241 0.52568895 0.15009357]\n",
      " [0.70860156 0.06271984 0.80692653 ... 0.01502953 0.94324721 0.39612645]]\n"
     ]
    }
   ],
   "source": [
    "print(new_data.shape)\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6597467  0.6025914  0.8249438  0.43570042 0.7246838 ]\n",
      " [0.3483673  0.4422352  0.33449763 0.7574107  0.21726435]\n",
      " [0.399386   0.36747652 0.41357148 0.5511156  0.32902807]\n",
      " [0.48720127 0.5554147  0.75285846 0.51367515 0.48262638]\n",
      " [0.58317345 0.5364338  0.70479494 0.70570993 0.5792106 ]\n",
      " [0.48714116 0.5002279  0.50343275 0.5767022  0.37495872]\n",
      " [0.62825197 0.48519695 0.1948639  0.33512163 0.29832935]\n",
      " [0.9774969  0.4512077  0.40989625 0.67284936 0.38486385]\n",
      " [0.68472224 0.54990464 0.6280487  0.48447376 0.48439556]\n",
      " [0.4271128  0.39516115 0.4694422  0.4509844  0.30254745]\n",
      " [0.4919806  0.52435994 0.70568174 0.59124166 0.6220854 ]\n",
      " [0.57235366 0.4323614  0.45119953 0.48169506 0.34668344]\n",
      " [0.4412004  0.43112126 0.49552327 0.41809094 0.5053555 ]\n",
      " [0.41655818 0.26287305 0.20786095 0.4033131  0.30010697]\n",
      " [0.40050176 0.38480565 0.24292482 0.51762146 0.1934618 ]\n",
      " [0.43261978 0.2417041  0.45136803 0.52725464 0.25675115]\n",
      " [0.5595094  0.3004853  0.3798677  0.37706771 0.4537416 ]\n",
      " [0.58798975 0.66218185 0.41450265 0.6657601  0.38566762]\n",
      " [0.30226204 0.5602418  0.8041491  0.6991346  0.6700363 ]\n",
      " [0.5419613  0.47787178 0.6118843  0.4601746  0.35815722]\n",
      " [0.48108348 0.4570001  0.3747465  0.45920703 0.44316632]\n",
      " [0.38362506 0.44023687 0.6875128  0.8189926  0.49001098]\n",
      " [0.48921517 0.3075072  0.21591616 0.6812611  0.34379637]\n",
      " [0.42184523 0.6078677  0.683805   0.2241266  0.4249454 ]\n",
      " [0.59111273 0.6323756  0.55239123 0.61090523 0.43979883]\n",
      " [0.7732512  0.61875224 0.51698077 0.6597406  0.65935206]\n",
      " [0.442639   0.43013328 0.41152442 0.7472258  0.39856547]\n",
      " [0.37294465 0.36230564 0.52146    0.5057599  0.4084963 ]\n",
      " [0.5004167  0.4623277  0.55869275 0.46860123 0.48705006]\n",
      " [0.54497296 0.57780313 0.38767847 0.41792184 0.3003901 ]\n",
      " [0.38998714 0.5017493  0.7055858  0.5652185  0.39805144]\n",
      " [0.40452063 0.33662122 0.4992602  0.4253704  0.37618768]\n",
      " [0.70356196 0.3877424  0.3705173  0.52775735 0.44996315]\n",
      " [0.7231463  0.39526245 0.6017809  0.30495912 0.51079667]\n",
      " [0.53012645 0.49319    0.40228587 0.4546023  0.5010582 ]\n",
      " [0.5265443  0.46549895 0.56172407 0.504646   0.49116158]\n",
      " [0.51776695 0.49088457 0.81912553 0.3703926  0.58446294]\n",
      " [0.10942832 0.35957158 0.80979294 0.7466114  0.4537372 ]\n",
      " [0.33468112 0.5311595  0.33930606 0.5943036  0.5060455 ]\n",
      " [0.81048316 0.65921324 0.48741603 0.74091613 0.5867561 ]\n",
      " [0.64591414 0.46483594 0.5364632  0.73153275 0.3647779 ]\n",
      " [0.64448714 0.5654526  0.6309678  0.6174998  0.34284493]\n",
      " [0.25612494 0.16238196 0.32647157 0.4939085  0.19308019]\n",
      " [0.25060496 0.24576032 0.59781885 0.60529757 0.54515964]\n",
      " [0.7080701  0.35969967 0.61756575 0.4283861  0.48581314]\n",
      " [0.41606852 0.48135248 0.92806596 0.3081737  0.51142186]\n",
      " [0.36754474 0.38964504 0.42250532 0.424567   0.25709352]\n",
      " [0.6236117  0.42658907 0.54778665 0.65790254 0.42605758]\n",
      " [0.5475185  0.5813738  0.49107504 0.5636818  0.6677219 ]\n",
      " [0.46276256 0.40235    0.64884144 0.3365385  0.48315018]\n",
      " [0.62565666 0.4128248  0.6437659  0.48288542 0.56598896]\n",
      " [0.30357164 0.27347282 0.59719557 0.48524898 0.23907948]\n",
      " [0.5449661  0.4856499  0.5961226  0.67427    0.5924239 ]\n",
      " [0.41573688 0.51466995 0.48187077 0.35058695 0.54084927]\n",
      " [0.48801884 0.39674428 0.54988045 0.6132857  0.38425273]\n",
      " [0.5361578  0.58627695 0.6993636  0.5262362  0.51880425]\n",
      " [0.5971781  0.62369037 0.75532687 0.27518496 0.62429446]\n",
      " [0.26496834 0.40915617 0.5622282  0.3958217  0.48393172]\n",
      " [0.54576665 0.40445143 0.6262156  0.63454777 0.4532472 ]\n",
      " [0.52841985 0.42246538 0.44970208 0.7015651  0.21514374]\n",
      " [0.7883641  0.4209013  0.4251026  0.4779931  0.3480209 ]\n",
      " [0.56797194 0.51222545 0.3435232  0.69441336 0.3329404 ]\n",
      " [0.4532444  0.49635145 0.5086836  0.51418024 0.4167521 ]\n",
      " [0.76814497 0.6136074  0.5219597  0.47927558 0.5357809 ]\n",
      " [0.76485056 0.5660307  0.39805934 0.7037656  0.74820614]\n",
      " [0.8242186  0.49516857 0.4094265  0.51770467 0.45943803]\n",
      " [0.3600217  0.47152033 0.71611005 0.45969868 0.47326374]\n",
      " [0.3835142  0.51332366 0.66406715 0.35273576 0.40183377]\n",
      " [0.5507321  0.50187254 0.581853   0.7396007  0.56889635]\n",
      " [0.42976725 0.30873293 0.4449914  0.4290698  0.42921063]\n",
      " [0.63078994 0.67075527 0.6612416  0.71606475 0.54694086]\n",
      " [0.26743478 0.67278385 0.77032846 0.259992   0.5154945 ]\n",
      " [0.5642553  0.4209913  0.6225793  0.63282204 0.49357402]\n",
      " [0.52914745 0.4238401  0.42083496 0.76305324 0.55221015]\n",
      " [0.3295547  0.70564365 0.7617207  0.37524766 0.48451155]\n",
      " [0.6427999  0.4726644  0.344881   0.5198143  0.14352286]\n",
      " [0.67415345 0.25747925 0.42178708 0.55892044 0.2885504 ]\n",
      " [0.7153656  0.59171486 0.43273526 0.5427635  0.53040713]\n",
      " [0.7342367  0.5852808  0.82059485 0.57494396 0.66861695]\n",
      " [0.38947344 0.4438033  0.57290655 0.33557406 0.29592788]\n",
      " [0.50247896 0.6356137  0.50751656 0.41536632 0.3295143 ]\n",
      " [0.6363105  0.53577363 0.26442522 0.5297062  0.32218933]\n",
      " [0.67282635 0.652715   0.7130072  0.53913367 0.6510483 ]\n",
      " [0.7333533  0.66206634 0.7412987  0.43093106 0.46983844]\n",
      " [0.26675075 0.4161441  0.31900626 0.44052982 0.36989814]\n",
      " [0.38434672 0.40012854 0.48530668 0.21103328 0.26057675]\n",
      " [0.62422115 0.5548085  0.44460046 0.78541356 0.350829  ]\n",
      " [0.69618976 0.38708675 0.46800822 0.45792085 0.32456097]\n",
      " [0.4908429  0.4672908  0.57375914 0.47701034 0.47304386]\n",
      " [0.46344823 0.3074954  0.5822881  0.4217587  0.40699127]\n",
      " [0.55677384 0.6463051  0.48738378 0.47272664 0.40402788]\n",
      " [0.3747356  0.36856708 0.5695789  0.464521   0.35412893]\n",
      " [0.68968445 0.5603126  0.666423   0.31005237 0.5472581 ]\n",
      " [0.6365582  0.73040676 0.3391015  0.66284156 0.41096357]\n",
      " [0.4998577  0.55871165 0.37083593 0.54352945 0.2961693 ]\n",
      " [0.28592017 0.38464415 0.4849801  0.5444274  0.39067137]\n",
      " [0.41557774 0.5273832  0.6966876  0.44566888 0.4238572 ]\n",
      " [0.5536741  0.52386546 0.5030388  0.4600591  0.47601956]\n",
      " [0.67931074 0.2843456  0.4587003  0.16077279 0.16686627]\n",
      " [0.3625917  0.3891267  0.40381134 0.6682815  0.37557   ]]\n"
     ]
    }
   ],
   "source": [
    "encoded_output.shape\n",
    "print(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs after TF setup https://www.tensorflow.org/install/pip \n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install mne\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, platform\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "mne.__version__\n",
    "from mne.viz import plot_alignment, snapshot_brain_montage\n",
    "import shutil\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from mne_bids import write_raw_bids, BIDSPath, print_dir_tree, make_dataset_description\n",
    "# from mne_bids.stats import count_events\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_utils = '/decoding_toolbox_py/helper_funcs' \n",
    "sys.path.append(path_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' VARIABLES '''\n",
    "\n",
    "dataset = 'eeg'\n",
    "\n",
    "amount_of_subjects = 1 # Change the range so the process is faster\n",
    "if amount_of_subjects > 26: amount_of_subjects = 26\n",
    "\n",
    "numC = 8\n",
    "\n",
    "angles = [i * 180./numC for i in range(numC)]\n",
    "\n",
    "x_labels = np.array(angles)\n",
    "\n",
    "resample = True # speeds up the procees but showing worse results overall\n",
    "if resample: resample_frequency = 20 # in Hz, original freq is 500Hz\n",
    "\n",
    "cfg_stim = dict()\n",
    "cfg_stim['kappa'] = 4\n",
    "cfg_stim['NumC'] = numC\n",
    "cfg_stim['Tuning'] = 'vonmises'\n",
    "# cfg_stim['Tuning'] = 'halfRectCos'\n",
    "cfg_stim['offset'] = 0\n",
    "\n",
    "cfg_train = dict()\n",
    "cfg_train['gamma'] = 0.1\n",
    "cfg_train['demean'] = True\n",
    "cfg_train['returnPattern'] = True\n",
    "\n",
    "cfg_test = dict()\n",
    "cfg_test['demean'] = 'traindata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.methods import read_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epochs, all_rawdata = read_data (task = 'main', amount_of_subjects=1, resample=False, resample_frequency=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'subj', 'nblock', 'ntrial', 'nrep', 'trial_type', 'cond-1',\n",
       "       'cond', 'rDV', 'DV', 'resp', 'deci-2', 'deci-1', 'deci', 'corr-1',\n",
       "       'r_map', 'correct', 'confi', 'RT', 'd1', 'conf_lvl', 'correct-1', 'd2',\n",
       "       'd3', 'd4', 'd5', 'd6', 'o1', 'o2', 'o3', 'o4', 'o5', 'o6', 'confi-1',\n",
       "       'conf_lvl-1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rawdata[0]['metadata'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 32, 2876) (250,)\n",
      "(250, 32, 2876) (250,)\n"
     ]
    }
   ],
   "source": [
    "neural_network = True\n",
    "transpose = False\n",
    "if neural_network:\n",
    "    X = all_rawdata[0]['epoch_dat']\n",
    "    y = all_rawdata[0]['metadata']['deci']\n",
    "    for i in range(1, amount_of_subjects):\n",
    "        X = np.concatenate((X, all_rawdata[i]['epoch_dat']), axis=0)\n",
    "        y = np.concatenate((y, all_rawdata[i]['metadata']['deci']))\n",
    "    print(X.shape, y.shape)\n",
    "    if transpose: \n",
    "        X = np.einsum('ijk->ikj', X)\n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "    numF, numC, numT = X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp39-cp39-macosx_10_15_x86_64.whl (230.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.1/230.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.4.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-macosx_10_14_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (22.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.23.2-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.2.0)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-macosx_10_9_x86_64.whl (26.7 MB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Requirement already satisfied: setuptools in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (65.6.3)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting numpy<1.24,>=1.22\n",
      "  Using cached numpy-1.23.5-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.12.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow) (1.7.3)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.6 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.0.3)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.2-cp39-cp39-macosx_10_10_universal2.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.33.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.19.1-py2.py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.6->jax>=0.3.15->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Collecting numpy<1.24,>=1.22\n",
      "  Downloading numpy-1.22.4-cp39-cp39-macosx_10_15_x86_64.whl (17.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/junjieji/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.12-py3-none-any.whl size=1498447 sha256=b9a7b72805caa12d2384ca8ec71f46578be79abf7702bc7b1297dcbad0b00ae1\n",
      "  Stored in directory: /Users/junjieji/Library/Caches/pip/wheels/76/d7/a7/8a37c82093e147abdd6ef1ec7b38276416d81ab59c3181abc1\n",
      "Successfully built jax\n",
      "Installing collected packages: libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, protobuf, numpy, keras, grpcio, google-pasta, gast, astunparse, opt-einsum, ml-dtypes, google-auth, jax, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.33.0\n",
      "    Uninstalling google-auth-1.33.0:\n",
      "      Successfully uninstalled google-auth-1.33.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.4.1\n",
      "    Uninstalling google-auth-oauthlib-0.4.1:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.4.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.6.0\n",
      "    Uninstalling tensorboard-2.6.0:\n",
      "      Successfully uninstalled tensorboard-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "streamlit 1.16.0 requires protobuf<4,>=3.12, but you have protobuf 4.23.2 which is incompatible.\n",
      "rdt 1.2.1 requires pyyaml<6,>=5.4.1, but you have pyyaml 6.0 which is incompatible.\n",
      "mlflow 1.29.0 requires packaging<22, but you have packaging 22.0 which is incompatible.\n",
      "google-cloud-storage 1.31.0 requires google-auth<2.0dev,>=1.11.0, but you have google-auth 2.19.1 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.19.1 which is incompatible.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.19.1 which is incompatible.\n",
      "ctgan 0.5.2 requires packaging<22,>=20, but you have packaging 22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astunparse-1.6.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.19.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 jax-0.4.12 keras-2.12.0 libclang-16.0.0 ml-dtypes-0.2.0 numpy-1.22.4 opt-einsum-3.3.0 protobuf-4.23.2 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(units=128, return_sequences=True, input_shape=(numC, numT))) \n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(units=64))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units=1, activation='sigmoid'))  \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF all subjects 80-20 = 0.556809024979855\n",
    "\n",
    "RF, Logistic, SVC = 1 sub 0.66\n",
    "\n",
    "XGB 1 sub 0.62\n",
    "\n",
    "XGB 26 0.5495568090249798\n",
    "\n",
    "Ada 0.6\n",
    "\n",
    "GB 0.62\n",
    "\n",
    "0.66\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catboost and RF both on more data with stacked are perfoming the same\n",
    "\n",
    "next im gonna add labels for the subjects\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
